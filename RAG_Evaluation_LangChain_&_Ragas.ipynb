{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c16eebde864c4e999531c10f250a025b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a195199e7f324a46b05c667535d3cab6",
              "IPY_MODEL_18fbd19b5b7f4d09a7fc84b0519aee74",
              "IPY_MODEL_ef220b4219e04b2ab2a91a2da87eae03"
            ],
            "layout": "IPY_MODEL_af36e762c13f4d6097c474267d18c779"
          }
        },
        "a195199e7f324a46b05c667535d3cab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fce40f05eb24ee38a599bd2cf2b7a78",
            "placeholder": "​",
            "style": "IPY_MODEL_2df51484c84e479bba58209a57a281f3",
            "value": "embedding nodes:  99%"
          }
        },
        "18fbd19b5b7f4d09a7fc84b0519aee74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c16089ecef6458ead3a586fbf1133c9",
            "max": 272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7868decb18ac44b4b6a7d32bd3f409b8",
            "value": 272
          }
        },
        "ef220b4219e04b2ab2a91a2da87eae03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ea21b3963d2404a9251c069e547000d",
            "placeholder": "​",
            "style": "IPY_MODEL_95b384a696f84fb1bd233d54d4f50099",
            "value": " 269/272 [00:11&lt;00:00, 28.55it/s]"
          }
        },
        "af36e762c13f4d6097c474267d18c779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "0fce40f05eb24ee38a599bd2cf2b7a78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df51484c84e479bba58209a57a281f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c16089ecef6458ead3a586fbf1133c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7868decb18ac44b4b6a7d32bd3f409b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ea21b3963d2404a9251c069e547000d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95b384a696f84fb1bd233d54d4f50099": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10336d5b31034cf685543a45efad7a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc29760a4bc74871b00126af2a3365c5",
              "IPY_MODEL_f19c799a59384e80959ff59bac4dff1e",
              "IPY_MODEL_7312503f6aa14cf2bb03042adee39a43"
            ],
            "layout": "IPY_MODEL_2488adbbad4f4fca9a67aa8cf5d6602e"
          }
        },
        "fc29760a4bc74871b00126af2a3365c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c455151372764036b1ef14b9c5704eef",
            "placeholder": "​",
            "style": "IPY_MODEL_24db57fd6e104ccc8161c5e2eb94a4fa",
            "value": "Generating: 100%"
          }
        },
        "f19c799a59384e80959ff59bac4dff1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d66ec6beeb0d4e719bd629c7ee442d8f",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40544c70b21740489fa911c8ec16c9e1",
            "value": 9
          }
        },
        "7312503f6aa14cf2bb03042adee39a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d02b3678fb84bc380501df23dee4138",
            "placeholder": "​",
            "style": "IPY_MODEL_ea08de901e9b4535addf6c1a03970bbb",
            "value": " 9/9 [01:22&lt;00:00,  8.65s/it]"
          }
        },
        "2488adbbad4f4fca9a67aa8cf5d6602e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c455151372764036b1ef14b9c5704eef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24db57fd6e104ccc8161c5e2eb94a4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d66ec6beeb0d4e719bd629c7ee442d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40544c70b21740489fa911c8ec16c9e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d02b3678fb84bc380501df23dee4138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea08de901e9b4535addf6c1a03970bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e402f29fe3204ad194f2b998ca382e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d925fdb477d643ab8ab5289e45e61033",
              "IPY_MODEL_b1d81707aa7b4328aff583a48cb5d14d",
              "IPY_MODEL_50d1798c287646e5b9da408695ca5065"
            ],
            "layout": "IPY_MODEL_273809a7e9204cba823d012ca7a4d168"
          }
        },
        "d925fdb477d643ab8ab5289e45e61033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01735d5f3a684ebba740df59507c5834",
            "placeholder": "​",
            "style": "IPY_MODEL_8ae70f2c9d084d8a8355711df9afdf08",
            "value": "Evaluating: 100%"
          }
        },
        "b1d81707aa7b4328aff583a48cb5d14d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2a24977a8ee44b1a2646a946d668feb",
            "max": 155,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72362aa3a0954ea88b71e3889937f240",
            "value": 155
          }
        },
        "50d1798c287646e5b9da408695ca5065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24adb77b046e45a48cc862083c5ccdf6",
            "placeholder": "​",
            "style": "IPY_MODEL_4131c35348524821bed81d6ac986c81f",
            "value": " 155/155 [06:06&lt;00:00,  4.87s/it]"
          }
        },
        "273809a7e9204cba823d012ca7a4d168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01735d5f3a684ebba740df59507c5834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ae70f2c9d084d8a8355711df9afdf08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2a24977a8ee44b1a2646a946d668feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72362aa3a0954ea88b71e3889937f240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24adb77b046e45a48cc862083c5ccdf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4131c35348524821bed81d6ac986c81f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa41fd9f5c1e46eba2bc36bbc707fde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84877198039e4d4da47f20311bca2db9",
              "IPY_MODEL_fdd08fd970194f378ea707a7e7b06c4e",
              "IPY_MODEL_77d7f39193914fe0856160008cd1e0da"
            ],
            "layout": "IPY_MODEL_82a6cf502efd4ecb9c86a64835ffad97"
          }
        },
        "84877198039e4d4da47f20311bca2db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7935378123b4ac6afbea7feb6a6565d",
            "placeholder": "​",
            "style": "IPY_MODEL_7a1ca835565e4923a10b387f06a38a99",
            "value": "Evaluating: 100%"
          }
        },
        "fdd08fd970194f378ea707a7e7b06c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78d4801e84aa4404a85ef3058e491f05",
            "max": 155,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7edd2fdd76249068c2131d6eb23b439",
            "value": 155
          }
        },
        "77d7f39193914fe0856160008cd1e0da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3635860d6dfe4e05969ff33fa56f8337",
            "placeholder": "​",
            "style": "IPY_MODEL_ecfab36aa4c0466bb7358d575f0adbb5",
            "value": " 155/155 [07:03&lt;00:00,  3.75s/it]"
          }
        },
        "82a6cf502efd4ecb9c86a64835ffad97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7935378123b4ac6afbea7feb6a6565d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a1ca835565e4923a10b387f06a38a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78d4801e84aa4404a85ef3058e491f05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7edd2fdd76249068c2131d6eb23b439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3635860d6dfe4e05969ff33fa56f8337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecfab36aa4c0466bb7358d575f0adbb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emarhnuel/Insurance_Chatbot_evaluation/blob/main/RAG_Evaluation_LangChain_%26_Ragas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Evaluation with Langchain and RAGAS\n",
        "\n",
        "In the following notebook I will be exploring the following:\n",
        "\n",
        "- Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "- Evaluating our pipeline with the [Ragas](https://github.com/explodinggradients/ragas) library\n",
        "- Making an adjustment to our RAG pipeline\n",
        "- Evaluating our adjusted pipeline against our baseline\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wa8ykQk92aLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BN13TZlSCv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61cbe2e-cd2d-40ba-cf12-538ab9e746f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.1/86.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.8/169.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-openai langchain_core langchain-community langchainhub openai ragas tiktoken cohere faiss_cpu requests==2.31.0 tokenizers==0.19 pypdf2 unstructured langchain langchain_together"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  sentence_transformers  rank_bm25 > /dev/null"
      ],
      "metadata": {
        "id": "8To80hN61nuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "\n",
        "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lhqp5rUThG-",
        "outputId": "09fd6647-b60b-4cfd-d168-585731002c4e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your OpenAI Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building our RAG pipeline\n",
        "\n",
        "I will:\n",
        "\n",
        "- Create an Index\n",
        "- Use a LLM to generate responses based on the retrieved context\n",
        "\n",
        "Let's get started by creating our index."
      ],
      "metadata": {
        "id": "DV_BOewX8CW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading Data\n",
        "\n"
      ],
      "metadata": {
        "id": "RmFFThawK8lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "# Replace with the actual path to your Markdown file in Colab\n",
        "markdown_path = \"/content/policy-booklet-0923.md\"\n",
        "loader = UnstructuredMarkdownLoader(markdown_path)\n",
        "documents = loader.load()\n"
      ],
      "metadata": {
        "id": "DTDNFXaBSO2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcf2802-c902-422a-c696-b56ae01eda29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3dJYlBCIX_p",
        "outputId": "bd10a745-5dc7-4e2a-ec9a-b4fd2b235ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/policy-booklet-0923.md'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transforming Data\n",
        "\n",
        "Now that I have gotten my single document - let's split it into smaller pieces so I can more effectively leverage it with our retrieval chain!\n",
        "\n",
        "We'll start with the classic: `RecursiveCharacterTextSplitter`."
      ],
      "metadata": {
        "id": "oQUl3sbZK4_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200\n",
        ")\n",
        "\n",
        "documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "6Nt2E1xnLNgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "Let's confirm we've split our document."
      ],
      "metadata": {
        "id": "ilzwQxhiLcVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wRw6a4aLfWh",
        "outputId": "4a23a2ba-c237-40f5-d167-ed94b00c0ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading OpenAI Embeddings Model\n",
        "\n",
        "We'll need a process by which we can convert our text into vectors that allow us to compare to our query vector.\n",
        "\n",
        "Let's use OpenAI's `text-embedding-ada-002` for this task! (soon we'll be able to leverage OpenAI's newest embedding model which is waiting on an approved PR to be merged as we speak!)"
      ],
      "metadata": {
        "id": "eZ93HkYcMJwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\"\n",
        ")"
      ],
      "metadata": {
        "id": "JU6CrDVZMgKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a FAISS VectorStore\n",
        "\n",
        "Now that i have my documents - I'll need a place to store them alongside their embeddings.\n",
        "\n",
        "I will be using Meta's FAISS for this task."
      ],
      "metadata": {
        "id": "rVtZR9JPLtR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "978TWiCtMA0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a Retriever\n",
        "\n",
        "To complete my index, all that's left to do is expose my vectorstore as a retriever"
      ],
      "metadata": {
        "id": "Z7ht6bJX9PAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "xne8P5dQTUiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the Retriever\n",
        "\n",
        "Now that we've gone through the trouble of creating our retriever - let's see it in action!"
      ],
      "metadata": {
        "id": "sO_DFBVKNvNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_documents = retriever.invoke(\"How much will you pay if my car is damaged?\")"
      ],
      "metadata": {
        "id": "I9_ONxpnN0n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in retrieved_documents:\n",
        "  print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Za12yt4OBy1",
        "outputId": "6208c21a-314a-4171-eed6-0d5b79bc1027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content=\"Faqs How Much Will You Pay If My Car Is Damaged?\\n\\nWhere damage to your car is covered under your policy, we'll pay the cost of repairing or replacing your car up to its UK market value. This is the current value of your car at the time of the claim. It may be different to the amount you paid or any amount you provided when you insured your car with us.\\n\\nWho Is Covered To Drive Other Cars?\\n\\nYour certificate of motor insurance will show who has cover to drive other cars. We'll only cover injury to third parties, or damage caused to their property, not to the car being driven. See 'Section 1: Liability' on page 11. Am I covered if I leave my car unlocked or the keys in the car? We won't pay a claim for theft or attempted theft if your car is left:\\n\\nUnlocked.\\n\\nWith keys or key fobs in, on, or attached to the car.\\n\\nWith the engine running.\\n\\nWith a window or roof open.\\n\\nWhat's not included in my cover?\\n\\nWe don't cover things like:\\n\\nMechanical or electrical failure.\\n\\nWear and tear.\" metadata={'source': '/content/policy-booklet-0923.md'}\n",
            "page_content=\"A courtesy car may not be available on the day.\\n\\nYou can only drive the courtesy car within the territorial limits. It cannot be used in the Republic of Ireland.\\n\\nWhilst You Have Your Courtesy Car\\n\\nIf your insurance cover is third party, fire and theft, we'll also provide cover under sections 4 and 5 for the courtesy car. If you claim under these sections, you'll have to pay the first:\\n\\n£250 if you claim under section 4 for accidental damage.\\n\\n£115 if you claim under section 5 for windscreen replacement.\\n\\n£25 if you claim for windscreen repair.\\n\\nYou'Re Not Covered For\\n\\n8 If your car is written off, or is stolen and not recovered, you won't get a courtesy car.\\n\\nSection 4: Accidental Damage\\n\\nDamage to your car We'll put things right if your car is damaged.\\n\\nAccidental Damage To Your Car Included With: Essential Comp Comp+\\n\\nIf your car is accidentally damaged, we can choose to either:\\n\\nRepair - we'll repair the damage ourselves or pay to repair it.\" metadata={'source': '/content/policy-booklet-0923.md'}\n",
            "page_content=\"When Am I Covered?\\n\\nIf we're dealing with your claim under sections 2 or 4 of your policy, we'll arrange for a hire car company to provide you with a hire car when both of the following apply:\\n\\nYour car is damaged because of an accident, fire or theft, or if it's stolen and not recovered.\\n\\nThe loss or damage happens within the territorial limits.\\n\\nFor details of your cover in the Republic of Ireland, see 'Where you can drive' on page 31.\\n\\nHow Much Am I Covered For?\\n\\nIf your car can be repaired, and is driveable\\n\\nWe'll provide you with a hire car from the point your car goes in for repair:\\n\\nIf you use our approved repairer, until they've repaired your car.\\n\\nIf you use your own repairer, for up to 21 days in a row while they're repairing your car. \\nIf your car can be repaired, and is not driveable\\n\\nAs soon as you've confirmed that we can start the repair, we'll provide you with a hire car:\\n\\nIf you use our approved repairer, until they have repaired your car.\" metadata={'source': '/content/policy-booklet-0923.md'}\n",
            "page_content=\"Your cover is the same as a Comprehensive policy.\\n\\nWherever Comprehensive is mentioned in this policy booklet, this also applies to you.\\n\\nContents\\n\\nFAQs 3 Where you can drive 31 Glossary 4 Losses we don't cover 33 Making a claim 6 Other conditions you need to know about 36 What your cover includes 8 How the policy works 37 Section 1: Liability 11 Everything else 41 Section 2: Fire and theft 14 If you have a complaint 42 Section 3: Courtesy car 17 If you're in an accident 43 Section 4: Accidental damage 18 How to get in touch Back cover Section 5: Windscreen damage 20 Section 6: Personal benefits 21 Section 7: Motor Legal Cover 23 Section 8: Guaranteed Hire Car Plus 28 Section 9: Protected No Claim Discount 30\\n\\nFaqs How Much Will You Pay If My Car Is Damaged?\" metadata={'source': '/content/policy-booklet-0923.md'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a RAG Chain\n",
        "\n"
      ],
      "metadata": {
        "id": "D8MKsT6JTgCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating a Prompt Template\n",
        "\n",
        "There are a few different ways I could create a prompt template - I could create a custom template, as seen in the code below, or I will simply pull a prompt from the prompt hub! Let's look at an example of that!"
      ],
      "metadata": {
        "id": "hQ0rUmEHugVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "retrieval_qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
      ],
      "metadata": {
        "id": "_8u8fx3NuXbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieval_qa_prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEpJiw6MuXLw",
        "outputId": "d23cae73-689e-428b-b6eb-889557b11a56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer any use questions based solely on the context below:\n",
            "\n",
            "<context>\n",
            "{context}\n",
            "</context>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - the prompt template is simple - but we'll create our own to be a bit more specific!"
      ],
      "metadata": {
        "id": "iyq88IPFRGoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "ijSNkTAjTsep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting Up the Basic QA Chain\n",
        "\n",
        "Now we can instantiate the basic RAG chain!\n",
        "\n",
        "I'll use LCEL directly just to see an example of it\n",
        "\n",
        "I'll also ensure to pass-through our context - which is critical for RAGAS."
      ],
      "metadata": {
        "id": "BYHnPaXl-cvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ],
      "metadata": {
        "id": "-TsjUWjbUfbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "zO69de-F-oMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much will you pay if my car is damaged?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FS5NxC6UyU2",
        "outputId": "f4642f57-d105-4d2d-e16f-7478597db603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Where damage to your car is covered under your policy, we'll pay the cost of repairing or replacing your car up to its UK market value.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Are my electric car’s charging cables covered?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)\n",
        "print(result[\"context\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIuHVGPOO9P2",
        "outputId": "1c3e54bd-a979-4080-806b-d4f6a8a5592c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, your electric car's charging cables are covered under 'Section 2: Fire and theft' or 'Section 4: Accidental damage' of your policy.\n",
            "[Document(page_content=\"Can I Use My Car Abroad?\\n\\nIf you want to use your car abroad, your cover depends on the type of policy you have and where you're driving. You can find full details in 'Where you can drive' on page 31. You may need a Green Card if you're travelling abroad. If you need one, please get in touch before you travel. We also recommend you take a European Accident Statement with you. You can get one at churchill.com/eas-form.pdf\\n\\nAre My Electric Car'S Charging Cables Covered?\\n\\nYour home charger and charging cables are considered an accessory to your car. This means they're covered under 'Section 2: Fire and theft' or \\n'Section 4: Accidental damage' of your policy. You're also covered for any accidents to others involving your charging cables when they are attached to your car. For example, someone tripping over your cable, as long as you have taken due care to prevent such an accident. See 'Section 1: Liability' on page 11.\\n\\nIs My Electric Car Battery Covered?\", metadata={'source': '/content/policy-booklet-0923.md'}), Document(page_content=\"Is My Electric Car Battery Covered?\\n\\nYour car's battery is covered if it's damaged as a result of an insured incident. This cover applies whether your battery is owned or leased.\\n\\nGlossary About The Glossary\\n\\nWhen we use these words or terms in the policy they have these specific meanings (unless we say differently). These apply to your car insurance.\\n\\nPlease note: Section 7: Motor Legal Cover, Section 8: Guaranteed Hire Car Plus and 'Liability for automated cars in Great Britain' in Section 1: Liability also include additional words or terms that have specific meanings - you can find these at the start of the relevant sections.\\n\\nIf your policy includes Green Flag breakdown cover, please see your Green Flag policy booklet for details of the words and terms that apply to your breakdown cover.\", metadata={'source': '/content/policy-booklet-0923.md'}), Document(page_content=\"Accessories Parts or products specifically designed to be fitted to your car, including your electric car's charging cables and the charger installed at your home. We may treat some accessories as modifications, so please tell us about any changes to your car. Approved repairer A repairer in our network of contracted repairers who's approved by us to carry out repairs to your car following a claim under this policy. Approved windscreen supplier A repairer we have approved and authorised to repair or replace your car's windscreen. Automated car Your car where it is lawfully driving itself on roads or other public places in Great Britain. Please note that your car must be identified on the Secretary of State's list of motor vehicles that may safely drive themselves. This identification may be by type, information recorded in a registration document or in some other way.\\n\\nCar insurance details The document that:\\n\\nIdentifies the policyholder.\\n\\nSets out details of the cover chosen.\", metadata={'source': '/content/policy-booklet-0923.md'}), Document(page_content=\"Your cover is the same as a Comprehensive policy.\\n\\nWherever Comprehensive is mentioned in this policy booklet, this also applies to you.\\n\\nContents\\n\\nFAQs 3 Where you can drive 31 Glossary 4 Losses we don't cover 33 Making a claim 6 Other conditions you need to know about 36 What your cover includes 8 How the policy works 37 Section 1: Liability 11 Everything else 41 Section 2: Fire and theft 14 If you have a complaint 42 Section 3: Courtesy car 17 If you're in an accident 43 Section 4: Accidental damage 18 How to get in touch Back cover Section 5: Windscreen damage 20 Section 6: Personal benefits 21 Section 7: Motor Legal Cover 23 Section 8: Guaranteed Hire Car Plus 28 Section 9: Protected No Claim Discount 30\\n\\nFaqs How Much Will You Pay If My Car Is Damaged?\", metadata={'source': '/content/policy-booklet-0923.md'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see that there are some improvements I could make here.\n",
        "\n",
        "For now, let's switch gears to RAGAS to see how I can leverage that tool to provide us insight into how our pipeline is performing!"
      ],
      "metadata": {
        "id": "a-XYZueEP42k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ragas Evaluation\n",
        "\n",
        "Ragas is a powerful library that lets us evaluate our RAG pipeline by collecting input/output/context triplets and obtaining metrics relating to a number of different aspects of our RAG pipeline.\n",
        "\n",
        "I'll be evluating on every core metric today, but in order to do that - I'll need to create a test set. Luckily for me, Ragas can do that directly!"
      ],
      "metadata": {
        "id": "EOECHyzHRqDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Synthetic Test Set Generation\n",
        "\n",
        "I will leverage Ragas' [`Synthetic Test Data generation`](https://docs.ragas.io/en/stable/concepts/testset_generation.html) functionality to generate my own synthetic QC pairs - as well as a synthetic ground truth - quite easily!\n",
        "\n",
        "> NOTE: This process will use `gpt-3.5-turbo-16k` as the base generator and `gpt-4` as the critic"
      ],
      "metadata": {
        "id": "KqXQ0jweWJOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a new set of documents to ensure I am not accidentally creating a sample test set that favours our base model too much!"
      ],
      "metadata": {
        "id": "MIscd7wSzeWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 200\n",
        ")\n",
        "documents = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "nVk5SlU9znXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiAPYw-hz-zo",
        "outputId": "4e40868b-828b-4572-fad0-938b60296b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "\n",
        "# generator with openai models\n",
        "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
        "critic_llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm,\n",
        "    critic_llm,\n",
        "    embeddings\n",
        ")\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(documents, test_size=33, distributions={simple: 0.3, reasoning: 0.4, multi_context: 0.3}, raise_exceptions=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "c16eebde864c4e999531c10f250a025b",
            "a195199e7f324a46b05c667535d3cab6",
            "18fbd19b5b7f4d09a7fc84b0519aee74",
            "ef220b4219e04b2ab2a91a2da87eae03",
            "af36e762c13f4d6097c474267d18c779",
            "0fce40f05eb24ee38a599bd2cf2b7a78",
            "2df51484c84e479bba58209a57a281f3",
            "4c16089ecef6458ead3a586fbf1133c9",
            "7868decb18ac44b4b6a7d32bd3f409b8",
            "9ea21b3963d2404a9251c069e547000d",
            "95b384a696f84fb1bd233d54d4f50099",
            "10336d5b31034cf685543a45efad7a7f",
            "fc29760a4bc74871b00126af2a3365c5",
            "f19c799a59384e80959ff59bac4dff1e",
            "7312503f6aa14cf2bb03042adee39a43",
            "2488adbbad4f4fca9a67aa8cf5d6602e",
            "c455151372764036b1ef14b9c5704eef",
            "24db57fd6e104ccc8161c5e2eb94a4fa",
            "d66ec6beeb0d4e719bd629c7ee442d8f",
            "40544c70b21740489fa911c8ec16c9e1",
            "8d02b3678fb84bc380501df23dee4138",
            "ea08de901e9b4535addf6c1a03970bbb"
          ]
        },
        "id": "IXc6sMglSej_",
        "outputId": "3f5a77c8-dc1a-4942-c5e9-8d4a941922fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "embedding nodes:   0%|          | 0/272 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c16eebde864c4e999531c10f250a025b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:ragas.testset.docstore:Filename and doc_id are the same for all nodes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10336d5b31034cf685543a45efad7a7f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTFnnOAFFLum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testset.test_data[0]"
      ],
      "metadata": {
        "id": "RaCDdImVU15s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1313f3e-046e-458c-e9ed-7c03a30d8d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataRow(question='What is the coverage for car keys lost abroad with the Foreign Use Extension?', contexts=[\"Car Security\\n\\nWe'll provide cover to reprogram immobilisers, infrared handsets and alarms.\\n\\nCar Hire\\n\\nIf you can't drive your car because of lost or damaged car keys and have our Guaranteed Hire Car Plus cover, we'll extend this cover while you're unable to use your car. See 'Section 8: Guaranteed Hire Car Plus' on page 28.\\n\\nDriving Abroad\\n\\nWhile driving your car abroad, we'll cover your car keys if they are lost when:\\n\\nYou have Comprehensive cover and you've added Foreign Use Extension to your cover before you travel (this will be shown on your car insurance details).\\n\\nYou have Comprehensive Plus cover, where 90 days of Foreign Use Extension is included for each insured period.\\n\\nYou'll need to replace your car keys and send the receipts to us. We'll then reimburse the costs up to the amounts shown on page 8.\\n\\nYou'Re Not Covered For\\n\\n8 We don't cover any reduction in your car's market value because of lost keys.\"], ground_truth=\"While driving your car abroad, if you have Comprehensive cover and added the Foreign Use Extension before you travel, or if you have Comprehensive Plus cover with 90 days of Foreign Use Extension included, we'll cover your car keys if they are lost. You'll need to replace the keys and send the receipts to us for reimbursement up to the specified amounts.\", evolution_type='simple', metadata=[{'source': '/content/policy-booklet-0923.md'}])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = testset.to_pandas()\n",
        "\n",
        "# Save as CSV file in your Drive\n",
        "test_df.to_csv('testset.csv', index=False)"
      ],
      "metadata": {
        "id": "frvzu1YxX8kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load testset from drive\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/testset.csv'\n",
        "\n",
        "test_df = pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "Yec9Y-89XciE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Generating Responses with RAG Pipeline\n",
        "\n",
        "I have gotten some QC pairs, and some ground truths, let's evaluate the RAG pipeline using Ragas.\n",
        "\n",
        "The process is, again, quite straightforward - thanks to Ragas and LangChain!\n",
        "\n",
        "Let's start by extracting the questions and ground truths from the create testset.\n",
        "\n",
        "I will start by converting our test dataset into a Pandas DataFrame."
      ],
      "metadata": {
        "id": "vrPsVwUAWFWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "GFKMIY8IZU8m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c8fc5ab-b958-4e49-e6c3-0a78db310193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             question  \\\n",
              "0   What is the coverage for car keys lost abroad ...   \n",
              "1   What actions will be taken if fraud is discove...   \n",
              "2   How does the total case value impact the decis...   \n",
              "3   How does intentional damage affect insurance c...   \n",
              "4   What conditions are required for insurance to ...   \n",
              "5   What steps should be taken at an accident scen...   \n",
              "6   What number and info are needed for insurance ...   \n",
              "7   What areas does the 'Liability for automated c...   \n",
              "8   What support is provided for personal accident...   \n",
              "9   What laws apply to the contract between the po...   \n",
              "10  What assistance is provided for personal accid...   \n",
              "11  What are some examples of statutory and/or aut...   \n",
              "12  How can you choose an appointed representative...   \n",
              "13  What changes should you inform your car insura...   \n",
              "14  How does the insurance policy handle accidenta...   \n",
              "15  Why is it important to get agreement before in...   \n",
              "16  How does Protected No Claim Discount work in t...   \n",
              "17  How does sharing information about your behavi...   \n",
              "18  What happens to premium refund and credit agre...   \n",
              "19  How to settle excess on a claim and avoid incr...   \n",
              "20  How will Green Flag handle breakdown cover com...   \n",
              "21  What's the first step after an accident for sa...   \n",
              "22  Under what conditions can you settle or end yo...   \n",
              "23  How is market value determined in car insuranc...   \n",
              "24  What factors can affect your car insurance pol...   \n",
              "25  What services does the Motor Legal Helpline of...   \n",
              "26  How long will you have access to a hire car wh...   \n",
              "27  What legal expenses are not covered under Moto...   \n",
              "28  What happens if Direct Debit for Green Flag br...   \n",
              "29  Where is the Period of Insurance info on the p...   \n",
              "30  Who are the parties in the agreement with U K ...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [\"Car Security\\n\\nWe'll provide cover to repro...   \n",
              "1   [\"Fraud\\n\\nYou must be honest in your dealings...   \n",
              "2   ['The difficulty of the case. Cases which are ...   \n",
              "3   [\"Deliberate Damage\\n\\n✘ We won't cover any lo...   \n",
              "4   [\"What We'Ll Do\\n\\nWe'll replace your car with...   \n",
              "5   [\"Safety Comes First\\n\\nStop at the scene of t...   \n",
              "6   [\"Making A Claim\\n\\nIf you need to claim These...   \n",
              "7   [\"It also covers journeys between these places...   \n",
              "8   [\"The driver's details, if possible.\\n\\nThe na...   \n",
              "9   [\"This policy is evidence of the contract betw...   \n",
              "10  [\"The driver's details, if possible.\\n\\nThe na...   \n",
              "11  [\"The Motor Insurance Database\\n\\nInformation ...   \n",
              "12  [\"Take all reasonable precautions to prevent a...   \n",
              "13  [\"You change the address where your car is nor...   \n",
              "14  [\"Accidental Damage To Your Car Included With:...   \n",
              "15  [\"Get our agreement before instructing a barri...   \n",
              "16  [\"If You Have Essentials Or Comprehensive And ...   \n",
              "17  [\"Share information about your behaviour with ...   \n",
              "18  [\"If you've made a claim, and/or used your Gre...   \n",
              "19  [\"Your personal details.\\n\\nYour policy number...   \n",
              "20  [\"If You Have A Complaint How We Can Help\\n\\nI...   \n",
              "21  [\"Motor Legal Cover\\n\\nIf your complaint relat...   \n",
              "22  [\"You dismiss your appointed representative\\nw...   \n",
              "23  [\"Loss of any limb A limb severed at or above ...   \n",
              "24  [\"You change the address where your car is nor...   \n",
              "25  [\"Buying or selling your car.\\n\\nBuying or hir...   \n",
              "26  [\"As soon as you've confirmed that we can star...   \n",
              "27  [\"You'Re Not Covered For\\n\\nLiability 8 We won...   \n",
              "28  [\"For Green Flag breakdown cover:\\n\\nWe will n...   \n",
              "29  [\"the No Claim Discount (NCD) that is in use o...   \n",
              "30  [\"Share information about your behaviour with ...   \n",
              "\n",
              "                                         ground_truth evolution_type  \\\n",
              "0   While driving your car abroad, if you have Com...         simple   \n",
              "1   If fraud is discovered in relation to the insu...         simple   \n",
              "2   The total case value, which includes the poten...      reasoning   \n",
              "3   Intentional damage, which is deliberate acts b...      reasoning   \n",
              "4   The conditions required for insurance to repla...      reasoning   \n",
              "5   Stop at the scene of the accident, call the po...      reasoning   \n",
              "6   For insurance policy inquiries, you will need ...  multi_context   \n",
              "7   The 'Liability for automated cars in Great Bri...  multi_context   \n",
              "8   We'll help if you or your partner are accident...  multi_context   \n",
              "9   You and we may choose which law will apply to ...         simple   \n",
              "10  We'll help if you or your partner are accident...         simple   \n",
              "11  Some examples of statutory and/or authorised b...         simple   \n",
              "12  You can choose an appointed representative to ...         simple   \n",
              "13  If anyone covered by the policy passes their d...         simple   \n",
              "14  If your car is accidentally damaged, the insur...         simple   \n",
              "15  It is important to get agreement before instru...         simple   \n",
              "16  The NCD owner will keep their No Claim Discoun...         simple   \n",
              "17  Sharing information about your behavior with o...         simple   \n",
              "18  We will not refund any car insurance premium i...      reasoning   \n",
              "19  You must not do, or refrain from doing, anythi...      reasoning   \n",
              "20  Green Flag aims to resolve most breakdown cove...      reasoning   \n",
              "21  Stop at the scene of the accident and if there...      reasoning   \n",
              "22  You can settle or end your claim without insur...      reasoning   \n",
              "23  Market value in car insurance is determined ba...      reasoning   \n",
              "24  Factors that can affect your car insurance pol...      reasoning   \n",
              "25  The Motor Legal Helpline offers assistance in ...  multi_context   \n",
              "26  You will have access to a hire car for up to 2...  multi_context   \n",
              "27  Legal costs for speeding offences, driving und...  multi_context   \n",
              "28  If you cancel your Direct Debit payments, this...  multi_context   \n",
              "29  The Period of Insurance information can be fou...  multi_context   \n",
              "30  This policy is a contract between the policyho...  multi_context   \n",
              "\n",
              "                                             metadata  episode_done  \n",
              "0     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "1     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "2     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "3     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "4     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "5     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "6     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "7   [{'source': '/content/policy-booklet-0923.md'}...          True  \n",
              "8   [{'source': '/content/policy-booklet-0923.md'}...          True  \n",
              "9     [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "10    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "11    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "12    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "13    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "14    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "15    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "16    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "17    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "18    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "19    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "20    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "21    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "22    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "23    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "24    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "25    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "26  [{'source': '/content/policy-booklet-0923.md'}...          True  \n",
              "27    [{'source': '/content/policy-booklet-0923.md'}]          True  \n",
              "28  [{'source': '/content/policy-booklet-0923.md'}...          True  \n",
              "29  [{'source': '/content/policy-booklet-0923.md'}...          True  \n",
              "30  [{'source': '/content/policy-booklet-0923.md'}...          True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c29f917-24f5-46e3-b8bd-bba38f566889\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>metadata</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the coverage for car keys lost abroad ...</td>\n",
              "      <td>[\"Car Security\\n\\nWe'll provide cover to repro...</td>\n",
              "      <td>While driving your car abroad, if you have Com...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What actions will be taken if fraud is discove...</td>\n",
              "      <td>[\"Fraud\\n\\nYou must be honest in your dealings...</td>\n",
              "      <td>If fraud is discovered in relation to the insu...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the total case value impact the decis...</td>\n",
              "      <td>['The difficulty of the case. Cases which are ...</td>\n",
              "      <td>The total case value, which includes the poten...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does intentional damage affect insurance c...</td>\n",
              "      <td>[\"Deliberate Damage\\n\\n✘ We won't cover any lo...</td>\n",
              "      <td>Intentional damage, which is deliberate acts b...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What conditions are required for insurance to ...</td>\n",
              "      <td>[\"What We'Ll Do\\n\\nWe'll replace your car with...</td>\n",
              "      <td>The conditions required for insurance to repla...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What steps should be taken at an accident scen...</td>\n",
              "      <td>[\"Safety Comes First\\n\\nStop at the scene of t...</td>\n",
              "      <td>Stop at the scene of the accident, call the po...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What number and info are needed for insurance ...</td>\n",
              "      <td>[\"Making A Claim\\n\\nIf you need to claim These...</td>\n",
              "      <td>For insurance policy inquiries, you will need ...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What areas does the 'Liability for automated c...</td>\n",
              "      <td>[\"It also covers journeys between these places...</td>\n",
              "      <td>The 'Liability for automated cars in Great Bri...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What support is provided for personal accident...</td>\n",
              "      <td>[\"The driver's details, if possible.\\n\\nThe na...</td>\n",
              "      <td>We'll help if you or your partner are accident...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What laws apply to the contract between the po...</td>\n",
              "      <td>[\"This policy is evidence of the contract betw...</td>\n",
              "      <td>You and we may choose which law will apply to ...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What assistance is provided for personal accid...</td>\n",
              "      <td>[\"The driver's details, if possible.\\n\\nThe na...</td>\n",
              "      <td>We'll help if you or your partner are accident...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What are some examples of statutory and/or aut...</td>\n",
              "      <td>[\"The Motor Insurance Database\\n\\nInformation ...</td>\n",
              "      <td>Some examples of statutory and/or authorised b...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>How can you choose an appointed representative...</td>\n",
              "      <td>[\"Take all reasonable precautions to prevent a...</td>\n",
              "      <td>You can choose an appointed representative to ...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What changes should you inform your car insura...</td>\n",
              "      <td>[\"You change the address where your car is nor...</td>\n",
              "      <td>If anyone covered by the policy passes their d...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>How does the insurance policy handle accidenta...</td>\n",
              "      <td>[\"Accidental Damage To Your Car Included With:...</td>\n",
              "      <td>If your car is accidentally damaged, the insur...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Why is it important to get agreement before in...</td>\n",
              "      <td>[\"Get our agreement before instructing a barri...</td>\n",
              "      <td>It is important to get agreement before instru...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>How does Protected No Claim Discount work in t...</td>\n",
              "      <td>[\"If You Have Essentials Or Comprehensive And ...</td>\n",
              "      <td>The NCD owner will keep their No Claim Discoun...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>How does sharing information about your behavi...</td>\n",
              "      <td>[\"Share information about your behaviour with ...</td>\n",
              "      <td>Sharing information about your behavior with o...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>What happens to premium refund and credit agre...</td>\n",
              "      <td>[\"If you've made a claim, and/or used your Gre...</td>\n",
              "      <td>We will not refund any car insurance premium i...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>How to settle excess on a claim and avoid incr...</td>\n",
              "      <td>[\"Your personal details.\\n\\nYour policy number...</td>\n",
              "      <td>You must not do, or refrain from doing, anythi...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>How will Green Flag handle breakdown cover com...</td>\n",
              "      <td>[\"If You Have A Complaint How We Can Help\\n\\nI...</td>\n",
              "      <td>Green Flag aims to resolve most breakdown cove...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What's the first step after an accident for sa...</td>\n",
              "      <td>[\"Motor Legal Cover\\n\\nIf your complaint relat...</td>\n",
              "      <td>Stop at the scene of the accident and if there...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Under what conditions can you settle or end yo...</td>\n",
              "      <td>[\"You dismiss your appointed representative\\nw...</td>\n",
              "      <td>You can settle or end your claim without insur...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>How is market value determined in car insuranc...</td>\n",
              "      <td>[\"Loss of any limb A limb severed at or above ...</td>\n",
              "      <td>Market value in car insurance is determined ba...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>What factors can affect your car insurance pol...</td>\n",
              "      <td>[\"You change the address where your car is nor...</td>\n",
              "      <td>Factors that can affect your car insurance pol...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>What services does the Motor Legal Helpline of...</td>\n",
              "      <td>[\"Buying or selling your car.\\n\\nBuying or hir...</td>\n",
              "      <td>The Motor Legal Helpline offers assistance in ...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>How long will you have access to a hire car wh...</td>\n",
              "      <td>[\"As soon as you've confirmed that we can star...</td>\n",
              "      <td>You will have access to a hire car for up to 2...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>What legal expenses are not covered under Moto...</td>\n",
              "      <td>[\"You'Re Not Covered For\\n\\nLiability 8 We won...</td>\n",
              "      <td>Legal costs for speeding offences, driving und...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}]</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>What happens if Direct Debit for Green Flag br...</td>\n",
              "      <td>[\"For Green Flag breakdown cover:\\n\\nWe will n...</td>\n",
              "      <td>If you cancel your Direct Debit payments, this...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Where is the Period of Insurance info on the p...</td>\n",
              "      <td>[\"the No Claim Discount (NCD) that is in use o...</td>\n",
              "      <td>The Period of Insurance information can be fou...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Who are the parties in the agreement with U K ...</td>\n",
              "      <td>[\"Share information about your behaviour with ...</td>\n",
              "      <td>This policy is a contract between the policyho...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': '/content/policy-booklet-0923.md'}...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c29f917-24f5-46e3-b8bd-bba38f566889')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c29f917-24f5-46e3-b8bd-bba38f566889 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c29f917-24f5-46e3-b8bd-bba38f566889');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5b88b694-10c6-402b-97f1-757f47f500c8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b88b694-10c6-402b-97f1-757f47f500c8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5b88b694-10c6-402b-97f1-757f47f500c8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2cedb55a-9d7e-4997-8a63-84a5dc48fbda\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2cedb55a-9d7e-4997-8a63-84a5dc48fbda button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"What legal expenses are not covered under Motor Legal Cover for liability, legal costs, and specific driving conditions?\",\n          \"Why is it important to get agreement before instructing a barrister or an expert witness in the context of a claim?\",\n          \"How is market value determined in car insurance based on replacement cost for a similar make, model, age, mileage, and condition?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"[\\\"For Green Flag breakdown cover:\\\\n\\\\nWe will not refund any Green Flag breakdown cover premium if you have made a Green Flag breakdown cover claim during the period of cover (regardless of whether you pay annually or by monthly instalments under a credit agreement).\\\\n\\\\nIf you pay by instalments under a credit agreement you must pay to us (1) all instalment payments that have already fallen due under the credit agreement and remain unpaid, and (2) the total remaining balance under the credit agreement.\\\\n\\\\nPlease Note\\\\n\\\\nIt's your responsibility to inform anyone insured under this policy that it has been cancelled.\\\\n\\\\nIf you cancel your Direct Debit payments, this won't cancel the policy. We'll ask you to pay the money you owe.\\\\n\\\\nThe cooling-off period is 14 days from the policy start date, or when you receive the policy documents, whichever is later.\\\\n\\\\nYou can find the administration fee mentioned above on your car insurance details.\\\", \\\"Your Right To Cancel The Policy Or Remove Any Optional Cover\\\\n\\\\nYou can cancel the policy, or remove any optional cover (including Green Flag breakdown cover) you've added, at any time - just get in touch with us. The text below explains whether we'll charge you and how much we'll refund in each situation. If the cancellation or removal happens before your cover starts We'll give a full refund. If the cancellation or removal happens during your 14-day cooling-off period We'll charge for the time you've had cover, and refund the rest of the premium paid. If the cancellation or removal happens after the 14-day cooling off period We'll charge for the time you've had cover, plus an administration fee, and refund any remaining premium paid.\\\\n\\\\nIf you've made a claim, and/or used your Green Flag cover, before the cancellation or removal happens For the policy and car insurance cover options, e.g. Protected No Claim Discount:\\\"]\",\n          \"[\\\"Get our agreement before instructing a barrister or an expert witness.\\\\n\\\\nKeep us and your appointed representative\\\\nup to date with any developments to do with the claim.\\\\n\\\\nAs soon as possible, give us and your appointed representative any information, evidence and documents that you have or know about.\\\\n\\\\nTell your appointed representative to give us any documents, information or advice that they have or know about, if we ask.\\\\n\\\\nYou must not take any action that hasn't been agreed by us or your appointed representative.\\\\nIf your appointed representative refuses to continue acting, or if you dismiss them If either of the following happens, we'll end cover for your costs immediately, unless we agree to a different appointed representative:\\\\n\\\\nYour appointed representative stops acting for you with good reason - for example, you behave dishonestly while dealing with your claim.\\\\n\\\\nYou dismiss your appointed representative\\\\nwithout good reason - for example, you disagree with their legal advice.\\\"]\",\n          \"[\\\"Loss of any limb A limb severed at or above the wrist or ankle, or the total and irrecoverable loss of use of a hand, arm, foot or leg. Main driver The person you declared was the main user of your car, and who's shown as the main driver on your car insurance details. Market value The cost of replacing your car with another of the same make and model, and of a similar age, mileage, and condition at the time of the accident or loss. Modifications Any changes to your car's standard specification, including optional extras. Modifications include changes to the appearance or the performance of your car, including wheels, suspension, bodywork and engine. Please note this is not a complete list. Modifications include changes made to your car by a previous owner. NCD owner The person who has earned the No Claim Discount (NCD) that is in use on this policy. Partner Your husband, wife, or civil partner, or someone you're living with as if you're married to them. Period of Insurance The length\\\"]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"If you cancel your Direct Debit payments, this won't cancel the policy. You will be asked to pay the money you owe. If the cancellation or removal of Green Flag breakdown cover happens after the 14-day cooling off period, you will be charged for the time you've had cover, plus an administration fee, and any remaining premium paid will be refunded.\",\n          \"The NCD owner will keep their No Claim Discount (NCD) if you make a claim, unless you make more than 2 claims in 3 years.\",\n          \"Factors that can affect your car insurance policy include changing the address where your car is normally kept overnight, changes in occupation of anyone covered by the policy, passing of driving test by anyone covered, changes in contact details like email address, changes in other policyholder details, incidents or motoring offences that have occurred since cover started such as motoring convictions, endorsements, penalty points, fixed penalties, speed camera offences, disqualifications, incidents, thefts, losses, insurance cancellations by another insurer for fraud or misrepresentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"evolution_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"simple\",\n          \"reasoning\",\n          \"multi_context\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[{'source': '/content/policy-booklet-0923.md'}, {'source': '/content/policy-booklet-0923.md'}]\",\n          \"[{'source': '/content/policy-booklet-0923.md'}]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"episode_done\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions = test_df[\"question\"].values.tolist()\n",
        "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
      ],
      "metadata": {
        "id": "xAiXbVmLYSoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I will generate responses using the RAG pipeline using the questions I have generated - I'll also need to collect the retrieved contexts for each question.\n",
        "\n",
        "I'll do this in a simple loop to see exactly what's happening!"
      ],
      "metadata": {
        "id": "aE5rfMLfbqKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "  answers.append(response[\"response\"].content)\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ],
      "metadata": {
        "id": "9_AayvT1dAQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I can wrap our information in a Hugging Face dataset for use in the Ragas library."
      ],
      "metadata": {
        "id": "opHaHmYDeBfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ],
      "metadata": {
        "id": "fY48YZITeHy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a peek and see what that looks like!"
      ],
      "metadata": {
        "id": "mmeVvQaZeogE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_dataset[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOpydvc8eqNM",
        "outputId": "83f24502-cb3e-4420-d064-b33e24a33066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What number and info are needed for insurance policy inquiries?',\n",
              " 'answer': 'The number needed for insurance policy inquiries is 0345 877 6680. The information needed includes personal details, policy number, car registration number, description of loss or damage, and details of the other driver if in an accident.',\n",
              " 'contexts': [\"Making A Claim\\n\\nIf you need to claim These steps will help you and enable us to process your claim quickly.\\n\\nHere are some important numbers you'll need if you have an accident\\n\\nNeed To Claim? 0345 878 6261 Windscreen Claims 0800 328 9150\\n\\nIf you have Essentials, Comprehensive or Comprehensive Plus cover\\n\\nMotor Legal Helpline 0345 246 2408\\n\\nIf you have Motor Legal Cover\\n\\nHelp With Anything Else 0345 877 6680\\n\\nStore these numbers in your phone so you have them available if needed. Even if you don't make a claim on your car, it's important to let us know about the accident as quickly as possible. This will enable us to contact the other party and resolve the entire claim, giving you the best service and keep the costs down.\\n\\nHow It Works\\n\\nTo get the ball rolling, we'll need to know things like:\\n\\nYour personal details.\\n\\nYour policy number.\\n\\nYour car registration number.\\n\\nA description of the loss or damage.\\n\\nIf you've been in an accident, the other driver's details.\",\n",
              "  \"Your personal details.\\n\\nYour policy number.\\n\\nYour car registration number.\\n\\nA description of the loss or damage.\\n\\nIf you've been in an accident, the other driver's details.\\n\\nPlease have these handy when you get in touch.\\n\\nOther Information You Need To Send Us\\n\\nIf you get any communication such as any notice or form from a court, any threat of legal action or similar, please contact us straight away. We'll deal with it or tell you what you need to do. You must also give us any other relevant information, documents or help we might need to process your claim, and pay any charges for sending such information. If you're unsure if a document is relevant, please give it to us anyway.\\n\\nAvoid Increasing The Amount Claimed\\n\\nYou must not do, or refrain from doing, anything that would increase the amount of the claim without our written permission. For example, admit liability for, or negotiate to settle, any claim.\\n\\nPaying The Excess\",\n",
              "  \"How The Policy Works Telling Us About Changes Before Your Cover Starts\\n\\nYou must tell us if anything has changed since you received your quote. For example, you must tell us if:\\n\\nAnything about your car changes.\\n\\nYou or any other driver have any claims or convictions that you haven't already told us about.\\n\\nThere are any changes to how your car is used, for example if you change from social, domestic and pleasure use to business use.\\n\\nThere are any modifications made to your car (see 'Modifications to your car' on page 36).\\n\\nYou want to add another driver to the policy, \\nor make any other change to who can drive your car.\\n\\nYou want to change to a higher level of cover, for example from Third Party, Fire and Theft to Comprehensive.\\n\\nAfter Your Cover Starts\\n\\nYou must tell us as soon as possible if:\\n\\nYou change the address where your car is normally kept overnight.\\n\\nAnyone covered by the policy changes their occupation.\\n\\nAnyone covered by the policy passes their driving test.\",\n",
              "  'How to get in touch Need to claim?\\n\\n0345 878 6261 Windscreen claims 0800 328 9150 If you have Essentials, Comprehensive or Comprehensive Plus cover Motor legal helpline 0345 246 2408 If you have Motor Legal Cover DriveSure help Email Support@churchill.com Call 0345 878 6375 If you have a question about telematics Help with anything else FAQs churchill.com/faqs Call 0345 877 6680 If you would like a Braille, large print or audio version\\n\\nof your documents, please let us know.\\n\\nChurchill insurance policies are underwritten by U K Insurance Limited, Registered office: The Wharf, Neville Street, Leeds, LS1 4AZ. Registered in England and Wales No.1179980. U K Insurance Limited is authorised by the Prudential Regulation Authority and regulated by the Financial Conduct Authority and the Prudential Regulation Authority. Calls may be recorded.\\n\\nB4C CH M PB 0923'],\n",
              " 'ground_truth': 'For insurance policy inquiries, you will need to provide your personal details, policy number, car registration number, and a description of the loss or damage.'}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating with Ragas\n",
        "\n",
        "Now that I have our response dataset - we can finally get into the  evaluation part of our Ragas framework!\n",
        "\n",
        "First, I'll import the desired metrics, then I can use them to evaluate my created dataset!\n",
        "\n",
        "Check out the specific metrics we'll be using in the Ragas documentation:\n",
        "\n",
        "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
        "- [Answer Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html)\n",
        "- [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html)\n",
        "- [Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html)\n",
        "- [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html)\n",
        "\n",
        "See the accompanied presentation for more in-depth explanations about each of the metrics!"
      ],
      "metadata": {
        "id": "xbsFm5FievJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_correctness,\n",
        "]"
      ],
      "metadata": {
        "id": "R2PXwyt8e5aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that's left to do is call \"evaluate\" and away we go!"
      ],
      "metadata": {
        "id": "Kx-vlsx_hrtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate(response_dataset, metrics, raise_exceptions=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e402f29fe3204ad194f2b998ca382e70",
            "d925fdb477d643ab8ab5289e45e61033",
            "b1d81707aa7b4328aff583a48cb5d14d",
            "50d1798c287646e5b9da408695ca5065",
            "273809a7e9204cba823d012ca7a4d168",
            "01735d5f3a684ebba740df59507c5834",
            "8ae70f2c9d084d8a8355711df9afdf08",
            "f2a24977a8ee44b1a2646a946d668feb",
            "72362aa3a0954ea88b71e3889937f240",
            "24adb77b046e45a48cc862083c5ccdf6",
            "4131c35348524821bed81d6ac986c81f"
          ]
        },
        "id": "DhlcfJ4lgYVI",
        "outputId": "55dafb7e-90ab-4bb5-ed7d-859d1284bfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/155 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e402f29fe3204ad194f2b998ca382e70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59886, Requested 1983. Please try again in 1.869s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59079, Requested 2121. Please try again in 1.2s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59481, Requested 1835. Please try again in 1.316s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59191, Requested 2074. Please try again in 1.265s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 58886, Requested 1676. Please try again in 562ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59025, Requested 2156. Please try again in 1.181s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59696, Requested 2169. Please try again in 1.865s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59648, Requested 2234. Please try again in 1.882s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59388, Requested 1786. Please try again in 1.174s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 58939, Requested 2188. Please try again in 1.127s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 58244, Requested 1888. Please try again in 132ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59104, Requested 2126. Please try again in 1.23s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59239, Requested 2114. Please try again in 1.353s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59506, Requested 2192. Please try again in 1.698s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqPArpSrgwDD",
        "outputId": "df0a27ac-5f62-4a27-febd-d416e7b1ca7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faithfulness': 0.8811, 'answer_relevancy': 0.9378, 'context_recall': 0.9087, 'context_precision': 0.8046, 'answer_correctness': 0.8744}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = results.to_pandas()\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2nsGzj8DhP9E",
        "outputId": "be4f0f47-fa36-4205-9be1-b35ed21b4b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             question  \\\n",
              "0   What is the coverage for car keys lost abroad ...   \n",
              "1   What actions will be taken if fraud is discove...   \n",
              "2   How does the total case value impact the decis...   \n",
              "3   How does intentional damage affect insurance c...   \n",
              "4   What conditions are required for insurance to ...   \n",
              "5   What steps should be taken at an accident scen...   \n",
              "6   What number and info are needed for insurance ...   \n",
              "7   What areas does the 'Liability for automated c...   \n",
              "8   What support is provided for personal accident...   \n",
              "9   What laws apply to the contract between the po...   \n",
              "10  What assistance is provided for personal accid...   \n",
              "11  What are some examples of statutory and/or aut...   \n",
              "12  How can you choose an appointed representative...   \n",
              "13  What changes should you inform your car insura...   \n",
              "14  How does the insurance policy handle accidenta...   \n",
              "15  Why is it important to get agreement before in...   \n",
              "16  How does Protected No Claim Discount work in t...   \n",
              "17  How does sharing information about your behavi...   \n",
              "18  What happens to premium refund and credit agre...   \n",
              "19  How to settle excess on a claim and avoid incr...   \n",
              "20  How will Green Flag handle breakdown cover com...   \n",
              "21  What's the first step after an accident for sa...   \n",
              "22  Under what conditions can you settle or end yo...   \n",
              "23  How is market value determined in car insuranc...   \n",
              "24  What factors can affect your car insurance pol...   \n",
              "25  What services does the Motor Legal Helpline of...   \n",
              "26  How long will you have access to a hire car wh...   \n",
              "27  What legal expenses are not covered under Moto...   \n",
              "28  What happens if Direct Debit for Green Flag br...   \n",
              "29  Where is the Period of Insurance info on the p...   \n",
              "30  Who are the parties in the agreement with U K ...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   The coverage for car keys lost abroad with the...   \n",
              "1   If fraud is discovered in relation to the insu...   \n",
              "2   The total case value impacts the decision to p...   \n",
              "3   Intentional damage caused by insured individua...   \n",
              "4   The conditions required for insurance to repla...   \n",
              "5   Stop at the scene of the accident, call the po...   \n",
              "6   The number needed for insurance policy inquiri...   \n",
              "7   The 'Liability for automated cars in Great Bri...   \n",
              "8   The support provided for personal accidents in...   \n",
              "9   English law applies to the contract between th...   \n",
              "10  If you or your partner are accidentally injure...   \n",
              "11  The police, the DVLA, the DVANI, the Insurance...   \n",
              "12  You can choose an appointed representative fro...   \n",
              "13  You should inform your car insurance provider ...   \n",
              "14  The insurance policy can choose to either repa...   \n",
              "15  It is important to get agreement before instru...   \n",
              "16  The NCD owner will keep their No Claim Discoun...   \n",
              "17  Sharing information about your behavior with o...   \n",
              "18  The premium will not be refunded and the remai...   \n",
              "19  To settle excess on a claim, you may need to p...   \n",
              "20  Green Flag will aim to resolve most breakdown ...   \n",
              "21  Stop at the scene of the accident and call the...   \n",
              "22  You can settle or end your claim without insur...   \n",
              "23  Market value in car insurance is determined ba...   \n",
              "24  Factors that can affect your car insurance pol...   \n",
              "25  The Motor Legal Helpline offers confidential l...   \n",
              "26  You will have access to a hire car for up to 2...   \n",
              "27  Legal expenses for speeding offences, driving ...   \n",
              "28  If Direct Debit payments for Green Flag breakd...   \n",
              "29  The Period of Insurance info can be found on t...   \n",
              "30  The parties in the agreement with U K Insuranc...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [You have Comprehensive cover and you've added...   \n",
              "1   [Fraud\\n\\nYou must be honest in your dealings ...   \n",
              "2   [The difficulty of the case. Cases which are m...   \n",
              "3   [Deliberate Damage\\n\\n✘ We won't cover any los...   \n",
              "4   [What We'Ll Do\\n\\nWe'll replace your car with ...   \n",
              "5   [Safety Comes First\\n\\nStop at the scene of th...   \n",
              "6   [Making A Claim\\n\\nIf you need to claim These ...   \n",
              "7   [Liability For Automated Cars In Great Britain...   \n",
              "8   [Section 6: Personal Benefits\\n\\nPersonal bene...   \n",
              "9   [This policy is evidence of the contract betwe...   \n",
              "10  [Personal Accident Included With: Essential Co...   \n",
              "11  [The Motor Insurance Database\\n\\nInformation r...   \n",
              "12  [Take all reasonable precautions to prevent a ...   \n",
              "13  [How The Policy Works Telling Us About Changes...   \n",
              "14  [Your cover is the same as a Comprehensive pol...   \n",
              "15  [If it would be reasonable to spend more in pu...   \n",
              "16  [No Claim Discount (Ncd)\\n\\nIf you don't claim...   \n",
              "17  [Share information about your behaviour with o...   \n",
              "18  [If you've made a claim, and/or used your Gree...   \n",
              "19  [Your personal details.\\n\\nYour policy number....   \n",
              "20  [If You Have A Complaint How We Can Help\\n\\nIf...   \n",
              "21  [Safety Comes First\\n\\nStop at the scene of th...   \n",
              "22  [You dismiss your appointed representative\\nwi...   \n",
              "23  [Third Party, Comprehensive Fire and Theft Ess...   \n",
              "24  [You change the address where your car is norm...   \n",
              "25  [Motor Legal Cover\\n\\nIf your complaint relate...   \n",
              "26  [When Am I Covered?\\n\\nIf we're dealing with y...   \n",
              "27  [You'Re Not Covered For\\n\\nLiability 8 We won'...   \n",
              "28  [For Green Flag breakdown cover:\\n\\nWe will no...   \n",
              "29  [No Claim Discount (Ncd)\\n\\nIf you don't claim...   \n",
              "30  [Share information about your behaviour with o...   \n",
              "\n",
              "                                         ground_truth  faithfulness  \\\n",
              "0   While driving your car abroad, if you have Com...           NaN   \n",
              "1   If fraud is discovered in relation to the insu...           NaN   \n",
              "2   The total case value, which includes the poten...           NaN   \n",
              "3   Intentional damage, which is deliberate acts b...      0.986342   \n",
              "4   The conditions required for insurance to repla...      0.982080   \n",
              "5   Stop at the scene of the accident, call the po...      0.912098   \n",
              "6   For insurance policy inquiries, you will need ...      0.869210   \n",
              "7   The 'Liability for automated cars in Great Bri...      0.997535   \n",
              "8   We'll help if you or your partner are accident...      0.920306   \n",
              "9   You and we may choose which law will apply to ...      0.963727   \n",
              "10  We'll help if you or your partner are accident...      0.954381   \n",
              "11  Some examples of statutory and/or authorised b...      0.474411   \n",
              "12  You can choose an appointed representative to ...      0.500000   \n",
              "13  If anyone covered by the policy passes their d...      1.000000   \n",
              "14  If your car is accidentally damaged, the insur...      1.000000   \n",
              "15  It is important to get agreement before instru...      0.973288   \n",
              "16  The NCD owner will keep their No Claim Discoun...      0.944923   \n",
              "17  Sharing information about your behavior with o...      1.000000   \n",
              "18  We will not refund any car insurance premium i...      0.924546   \n",
              "19  You must not do, or refrain from doing, anythi...      0.865559   \n",
              "20  Green Flag aims to resolve most breakdown cove...      0.880651   \n",
              "21  Stop at the scene of the accident and if there...      0.955277   \n",
              "22  You can settle or end your claim without insur...      0.928314   \n",
              "23  Market value in car insurance is determined ba...      1.000000   \n",
              "24  Factors that can affect your car insurance pol...      0.746788   \n",
              "25  The Motor Legal Helpline offers assistance in ...      1.000000   \n",
              "26  You will have access to a hire car for up to 2...      0.557123   \n",
              "27  Legal costs for speeding offences, driving und...      1.000000   \n",
              "28  If you cancel your Direct Debit payments, this...      0.500000   \n",
              "29  The Period of Insurance information can be fou...      1.000000   \n",
              "30  This policy is a contract between the policyho...      0.833333   \n",
              "\n",
              "    answer_relevancy  context_recall  context_precision  answer_correctness  \n",
              "0                NaN             NaN                NaN                 NaN  \n",
              "1                NaN             NaN                NaN                 NaN  \n",
              "2                NaN             NaN                NaN            1.000000  \n",
              "3           1.000000        1.000000           0.608746            1.000000  \n",
              "4           1.000000        0.805556           0.548485            0.857143  \n",
              "5           1.000000        1.000000           0.617023            1.000000  \n",
              "6           1.000000        1.000000           0.699986            1.000000  \n",
              "7           1.000000        1.000000           0.622987            1.000000  \n",
              "8           1.000000        1.000000           0.808368            0.833333  \n",
              "9           1.000000        1.000000           0.728863            1.000000  \n",
              "10          1.000000        0.478177           0.999999            1.000000  \n",
              "11          1.000000        0.994218           1.000000            0.469758  \n",
              "12          0.942875        1.000000           1.000000            0.539093  \n",
              "13          0.933852        1.000000           1.000000            0.723422  \n",
              "14          0.933552        1.000000           0.685823            1.000000  \n",
              "15          1.000000        1.000000           0.618485            1.000000  \n",
              "16          1.000000        1.000000           0.995723            1.000000  \n",
              "17          0.666667        1.000000           0.487873            1.000000  \n",
              "18          1.000000        1.000000           0.748959            0.250000  \n",
              "19          0.500000        0.916667           0.791004            1.000000  \n",
              "20          1.000000        1.000000           0.513524            1.000000  \n",
              "21          1.000000        1.000000           0.547548            1.000000  \n",
              "22          1.000000        1.000000           0.995417            0.869636  \n",
              "23          0.847882        0.000000           0.891678            1.000000  \n",
              "24          0.936174        0.916667           0.849050            1.000000  \n",
              "25          0.775748        0.333333           0.936577            1.000000  \n",
              "26          0.947360        1.000000           1.000000            0.748474  \n",
              "27          0.966179        1.000000           0.916667            0.998805  \n",
              "28          0.959763        1.000000           1.000000            0.555472  \n",
              "29          0.903968        1.000000           0.916667            0.746471  \n",
              "30          0.944894        1.000000           1.000000            0.766503  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-79f17515-d428-4440-9592-3df6e935bdd2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the coverage for car keys lost abroad ...</td>\n",
              "      <td>The coverage for car keys lost abroad with the...</td>\n",
              "      <td>[You have Comprehensive cover and you've added...</td>\n",
              "      <td>While driving your car abroad, if you have Com...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What actions will be taken if fraud is discove...</td>\n",
              "      <td>If fraud is discovered in relation to the insu...</td>\n",
              "      <td>[Fraud\\n\\nYou must be honest in your dealings ...</td>\n",
              "      <td>If fraud is discovered in relation to the insu...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the total case value impact the decis...</td>\n",
              "      <td>The total case value impacts the decision to p...</td>\n",
              "      <td>[The difficulty of the case. Cases which are m...</td>\n",
              "      <td>The total case value, which includes the poten...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How does intentional damage affect insurance c...</td>\n",
              "      <td>Intentional damage caused by insured individua...</td>\n",
              "      <td>[Deliberate Damage\\n\\n✘ We won't cover any los...</td>\n",
              "      <td>Intentional damage, which is deliberate acts b...</td>\n",
              "      <td>0.986342</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.608746</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What conditions are required for insurance to ...</td>\n",
              "      <td>The conditions required for insurance to repla...</td>\n",
              "      <td>[What We'Ll Do\\n\\nWe'll replace your car with ...</td>\n",
              "      <td>The conditions required for insurance to repla...</td>\n",
              "      <td>0.982080</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.548485</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What steps should be taken at an accident scen...</td>\n",
              "      <td>Stop at the scene of the accident, call the po...</td>\n",
              "      <td>[Safety Comes First\\n\\nStop at the scene of th...</td>\n",
              "      <td>Stop at the scene of the accident, call the po...</td>\n",
              "      <td>0.912098</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.617023</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What number and info are needed for insurance ...</td>\n",
              "      <td>The number needed for insurance policy inquiri...</td>\n",
              "      <td>[Making A Claim\\n\\nIf you need to claim These ...</td>\n",
              "      <td>For insurance policy inquiries, you will need ...</td>\n",
              "      <td>0.869210</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.699986</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What areas does the 'Liability for automated c...</td>\n",
              "      <td>The 'Liability for automated cars in Great Bri...</td>\n",
              "      <td>[Liability For Automated Cars In Great Britain...</td>\n",
              "      <td>The 'Liability for automated cars in Great Bri...</td>\n",
              "      <td>0.997535</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.622987</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What support is provided for personal accident...</td>\n",
              "      <td>The support provided for personal accidents in...</td>\n",
              "      <td>[Section 6: Personal Benefits\\n\\nPersonal bene...</td>\n",
              "      <td>We'll help if you or your partner are accident...</td>\n",
              "      <td>0.920306</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.808368</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What laws apply to the contract between the po...</td>\n",
              "      <td>English law applies to the contract between th...</td>\n",
              "      <td>[This policy is evidence of the contract betwe...</td>\n",
              "      <td>You and we may choose which law will apply to ...</td>\n",
              "      <td>0.963727</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.728863</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What assistance is provided for personal accid...</td>\n",
              "      <td>If you or your partner are accidentally injure...</td>\n",
              "      <td>[Personal Accident Included With: Essential Co...</td>\n",
              "      <td>We'll help if you or your partner are accident...</td>\n",
              "      <td>0.954381</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.478177</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What are some examples of statutory and/or aut...</td>\n",
              "      <td>The police, the DVLA, the DVANI, the Insurance...</td>\n",
              "      <td>[The Motor Insurance Database\\n\\nInformation r...</td>\n",
              "      <td>Some examples of statutory and/or authorised b...</td>\n",
              "      <td>0.474411</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.994218</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.469758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>How can you choose an appointed representative...</td>\n",
              "      <td>You can choose an appointed representative fro...</td>\n",
              "      <td>[Take all reasonable precautions to prevent a ...</td>\n",
              "      <td>You can choose an appointed representative to ...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.942875</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.539093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What changes should you inform your car insura...</td>\n",
              "      <td>You should inform your car insurance provider ...</td>\n",
              "      <td>[How The Policy Works Telling Us About Changes...</td>\n",
              "      <td>If anyone covered by the policy passes their d...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.933852</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.723422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>How does the insurance policy handle accidenta...</td>\n",
              "      <td>The insurance policy can choose to either repa...</td>\n",
              "      <td>[Your cover is the same as a Comprehensive pol...</td>\n",
              "      <td>If your car is accidentally damaged, the insur...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.933552</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.685823</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Why is it important to get agreement before in...</td>\n",
              "      <td>It is important to get agreement before instru...</td>\n",
              "      <td>[If it would be reasonable to spend more in pu...</td>\n",
              "      <td>It is important to get agreement before instru...</td>\n",
              "      <td>0.973288</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.618485</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>How does Protected No Claim Discount work in t...</td>\n",
              "      <td>The NCD owner will keep their No Claim Discoun...</td>\n",
              "      <td>[No Claim Discount (Ncd)\\n\\nIf you don't claim...</td>\n",
              "      <td>The NCD owner will keep their No Claim Discoun...</td>\n",
              "      <td>0.944923</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995723</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>How does sharing information about your behavi...</td>\n",
              "      <td>Sharing information about your behavior with o...</td>\n",
              "      <td>[Share information about your behaviour with o...</td>\n",
              "      <td>Sharing information about your behavior with o...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.487873</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>What happens to premium refund and credit agre...</td>\n",
              "      <td>The premium will not be refunded and the remai...</td>\n",
              "      <td>[If you've made a claim, and/or used your Gree...</td>\n",
              "      <td>We will not refund any car insurance premium i...</td>\n",
              "      <td>0.924546</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.748959</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>How to settle excess on a claim and avoid incr...</td>\n",
              "      <td>To settle excess on a claim, you may need to p...</td>\n",
              "      <td>[Your personal details.\\n\\nYour policy number....</td>\n",
              "      <td>You must not do, or refrain from doing, anythi...</td>\n",
              "      <td>0.865559</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.791004</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>How will Green Flag handle breakdown cover com...</td>\n",
              "      <td>Green Flag will aim to resolve most breakdown ...</td>\n",
              "      <td>[If You Have A Complaint How We Can Help\\n\\nIf...</td>\n",
              "      <td>Green Flag aims to resolve most breakdown cove...</td>\n",
              "      <td>0.880651</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.513524</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>What's the first step after an accident for sa...</td>\n",
              "      <td>Stop at the scene of the accident and call the...</td>\n",
              "      <td>[Safety Comes First\\n\\nStop at the scene of th...</td>\n",
              "      <td>Stop at the scene of the accident and if there...</td>\n",
              "      <td>0.955277</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.547548</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Under what conditions can you settle or end yo...</td>\n",
              "      <td>You can settle or end your claim without insur...</td>\n",
              "      <td>[You dismiss your appointed representative\\nwi...</td>\n",
              "      <td>You can settle or end your claim without insur...</td>\n",
              "      <td>0.928314</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.995417</td>\n",
              "      <td>0.869636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>How is market value determined in car insuranc...</td>\n",
              "      <td>Market value in car insurance is determined ba...</td>\n",
              "      <td>[Third Party, Comprehensive Fire and Theft Ess...</td>\n",
              "      <td>Market value in car insurance is determined ba...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.847882</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.891678</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>What factors can affect your car insurance pol...</td>\n",
              "      <td>Factors that can affect your car insurance pol...</td>\n",
              "      <td>[You change the address where your car is norm...</td>\n",
              "      <td>Factors that can affect your car insurance pol...</td>\n",
              "      <td>0.746788</td>\n",
              "      <td>0.936174</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.849050</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>What services does the Motor Legal Helpline of...</td>\n",
              "      <td>The Motor Legal Helpline offers confidential l...</td>\n",
              "      <td>[Motor Legal Cover\\n\\nIf your complaint relate...</td>\n",
              "      <td>The Motor Legal Helpline offers assistance in ...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.775748</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.936577</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>How long will you have access to a hire car wh...</td>\n",
              "      <td>You will have access to a hire car for up to 2...</td>\n",
              "      <td>[When Am I Covered?\\n\\nIf we're dealing with y...</td>\n",
              "      <td>You will have access to a hire car for up to 2...</td>\n",
              "      <td>0.557123</td>\n",
              "      <td>0.947360</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.748474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>What legal expenses are not covered under Moto...</td>\n",
              "      <td>Legal expenses for speeding offences, driving ...</td>\n",
              "      <td>[You'Re Not Covered For\\n\\nLiability 8 We won'...</td>\n",
              "      <td>Legal costs for speeding offences, driving und...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.966179</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.998805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>What happens if Direct Debit for Green Flag br...</td>\n",
              "      <td>If Direct Debit payments for Green Flag breakd...</td>\n",
              "      <td>[For Green Flag breakdown cover:\\n\\nWe will no...</td>\n",
              "      <td>If you cancel your Direct Debit payments, this...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.959763</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.555472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Where is the Period of Insurance info on the p...</td>\n",
              "      <td>The Period of Insurance info can be found on t...</td>\n",
              "      <td>[No Claim Discount (Ncd)\\n\\nIf you don't claim...</td>\n",
              "      <td>The Period of Insurance information can be fou...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903968</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.746471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Who are the parties in the agreement with U K ...</td>\n",
              "      <td>The parties in the agreement with U K Insuranc...</td>\n",
              "      <td>[Share information about your behaviour with o...</td>\n",
              "      <td>This policy is a contract between the policyho...</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.944894</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.766503</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79f17515-d428-4440-9592-3df6e935bdd2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-79f17515-d428-4440-9592-3df6e935bdd2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-79f17515-d428-4440-9592-3df6e935bdd2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0414fc6c-f6a3-43aa-b452-e84b7975f0c1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0414fc6c-f6a3-43aa-b452-e84b7975f0c1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0414fc6c-f6a3-43aa-b452-e84b7975f0c1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f3e0ad82-5d1e-45a0-89a4-ea4b171302e4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f3e0ad82-5d1e-45a0-89a4-ea4b171302e4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"What legal expenses are not covered under Motor Legal Cover for liability, legal costs, and specific driving conditions?\",\n          \"Why is it important to get agreement before instructing a barrister or an expert witness in the context of a claim?\",\n          \"How is market value determined in car insurance based on replacement cost for a similar make, model, age, mileage, and condition?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"Legal expenses for speeding offences, driving under the influence of alcohol or drugs, and parking offences are not covered under Motor Legal Cover for liability, legal costs, and specific driving conditions.\",\n          \"It is important to get agreement before instructing a barrister or an expert witness in the context of a claim because if the barrister or expert witness does not agree with you, you will have to pay for their advice.\",\n          \"Market value in car insurance is determined based on the cost of replacing the insured car with another of the same make and model, and of a similar age, mileage, and condition at the time of the accident or loss.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"If you cancel your Direct Debit payments, this won't cancel the policy. You will be asked to pay the money you owe. If the cancellation or removal of Green Flag breakdown cover happens after the 14-day cooling off period, you will be charged for the time you've had cover, plus an administration fee, and any remaining premium paid will be refunded.\",\n          \"The NCD owner will keep their No Claim Discount (NCD) if you make a claim, unless you make more than 2 claims in 3 years.\",\n          \"Factors that can affect your car insurance policy include changing the address where your car is normally kept overnight, changes in occupation of anyone covered by the policy, passing of driving test by anyone covered, changes in contact details like email address, changes in other policyholder details, incidents or motoring offences that have occurred since cover started such as motoring convictions, endorsements, penalty points, fixed penalties, speed camera offences, disqualifications, incidents, thefts, losses, insurance cancellations by another insurer for fraud or misrepresentation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1669123616265664,\n        \"min\": 0.4744105147292914,\n        \"max\": 1.0,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.8655587371099025,\n          0.5,\n          0.9863421007632972\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.11550161333063892,\n        \"min\": 0.5,\n        \"max\": 1.0,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          0.7757482990192235,\n          0.9661793898210882,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23845657528562178,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          1.0,\n          0.999999999975,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18097082081792196,\n        \"min\": 0.4878733829998434,\n        \"max\": 0.999999999975,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          0.7910044456298083,\n          0.999999999975,\n          0.6087463568498959\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.20000723490997324,\n        \"min\": 0.25,\n        \"max\": 1.0000000000000007,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          1.0000000000000007,\n          0.9988048976694497,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing a More Performant Retriever\n",
        "\n",
        "Now that I have established a baseline - I can see how any changes impact my pipeline's performance!\n",
        "\n",
        "Let's modify the retriever and see how that impacts our Ragas metrics!"
      ],
      "metadata": {
        "id": "MWfiu_pLh3JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import MultiQueryRetriever\n",
        "\n",
        "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=primary_qa_llm)"
      ],
      "metadata": {
        "id": "nKIuM336isBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'll also re-create the RAG pipeline using the abstractions that come packaged with LangChain v0.1.0!\n",
        "\n",
        "First, I will create a chain to \"stuff\" the documents into my context!"
      ],
      "metadata": {
        "id": "82rcj3L-i_c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain = create_stuff_documents_chain(primary_qa_llm, retrieval_qa_prompt)"
      ],
      "metadata": {
        "id": "EfdCgTw7jC4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create the retrieval chain!"
      ],
      "metadata": {
        "id": "ozYl5WdPnvLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
      ],
      "metadata": {
        "id": "9AK7wHVnn0U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"If my car can be repaired, and is driveable, what will happen?\"})"
      ],
      "metadata": {
        "id": "cmKORMfMoCjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICMsUWbWoOpf",
        "outputId": "773228ae-251a-48cd-a773-9a7d989baf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If your car can be repaired and is driveable, the insurance company will provide you with a hire car from the point your car goes in for repair. If you use their approved repairer, you will have the hire car until they have repaired your car. If you choose to use your own repairer, you will have the hire car for up to 21 days in a row while they are repairing your car.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, just from those responses this chain *feels* better - but lets see how it performs on the eval!\n",
        "\n",
        "I will do the same process like I did before to collect the pipeline's contexts and answers."
      ],
      "metadata": {
        "id": "OxkU0HdpoaiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = retrieval_chain.invoke({\"input\" : question})\n",
        "  answers.append(response[\"answer\"])\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ],
      "metadata": {
        "id": "kO8cWxn2oinT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now I can convert this into a dataset, just like we did before."
      ],
      "metadata": {
        "id": "tgagfhPUtM2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_dataset_advanced_retrieval = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ],
      "metadata": {
        "id": "5FcllGeSovP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate on the same metrics we did for the first pipeline and see how it does!"
      ],
      "metadata": {
        "id": "dELYabwktR2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "advanced_retrieval_results = evaluate(response_dataset_advanced_retrieval, metrics, raise_exceptions=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa41fd9f5c1e46eba2bc36bbc707fde3",
            "84877198039e4d4da47f20311bca2db9",
            "fdd08fd970194f378ea707a7e7b06c4e",
            "77d7f39193914fe0856160008cd1e0da",
            "82a6cf502efd4ecb9c86a64835ffad97",
            "f7935378123b4ac6afbea7feb6a6565d",
            "7a1ca835565e4923a10b387f06a38a99",
            "78d4801e84aa4404a85ef3058e491f05",
            "f7edd2fdd76249068c2131d6eb23b439",
            "3635860d6dfe4e05969ff33fa56f8337",
            "ecfab36aa4c0466bb7358d575f0adbb5"
          ]
        },
        "id": "d7uHseWJo2TU",
        "outputId": "ad22b0ca-0d89-47f9-a1b9-666bdc99fb29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/155 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa41fd9f5c1e46eba2bc36bbc707fde3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59053, Requested 2292. Please try again in 1.345s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59694, Requested 2616. Please try again in 2.31s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_correctness.py\", line 250, in _ascore\n",
            "    is_statement_present = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59039, Requested 1619. Please try again in 658ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 58628, Requested 2095. Please try again in 723ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 58376, Requested 2251. Please try again in 627ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59904, Requested 2301. Please try again in 2.205s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59254, Requested 2446. Please try again in 1.7s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59165, Requested 2423. Please try again in 1.588s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59688, Requested 2234. Please try again in 1.921s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59798, Requested 2427. Please try again in 2.225s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59726, Requested 2403. Please try again in 2.129s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 58654, Requested 2203. Please try again in 857ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59135, Requested 2162. Please try again in 1.297s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59303, Requested 2389. Please try again in 1.692s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59903, Requested 2029. Please try again in 1.932s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59549, Requested 2227. Please try again in 1.776s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59197, Requested 2614. Please try again in 1.811s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_answer_relevance.py\", line 152, in _ascore\n",
            "    result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59429, Requested 2299. Please try again in 1.728s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 58553, Requested 2469. Please try again in 1.022s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59217, Requested 2350. Please try again in 1.567s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59586, Requested 2464. Please try again in 2.05s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "ERROR:ragas.executor:Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 571, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/metrics/_faithfulness.py\", line 266, in _ascore\n",
            "    nli_result = await self.llm.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\n",
            "    return self.__get_result()\n",
            "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-VhZPlHc9jjxNNOfDr2zxXW9n on tokens per min (TPM): Limit 60000, Used 59539, Requested 2325. Please try again in 1.864s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing Results\n",
        "\n",
        "Now I can compare the results and see what directional changes occured!\n",
        "\n",
        "Let's refresh with the initial metrics."
      ],
      "metadata": {
        "id": "J0hzqq5VtZ2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WWGRaF5qx3V",
        "outputId": "eee31421-082d-4842-a944-804fb8fca72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faithfulness': 0.8811, 'answer_relevancy': 0.9378, 'context_recall': 0.9087, 'context_precision': 0.8046, 'answer_correctness': 0.8744}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see how the other advanced retrieval modified our chain!"
      ],
      "metadata": {
        "id": "oFv_yAeotmFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "advanced_retrieval_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpV11dxJo7xa",
        "outputId": "a612b3b8-3cc5-427e-f38f-cd05e46bc6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'faithfulness': 0.8053, 'answer_relevancy': 0.8226, 'context_recall': 0.9388, 'context_precision': 0.8830, 'answer_correctness': 0.8726}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_original = pd.DataFrame(list(results.items()), columns=['Metric', 'Baseline'])\n",
        "df_comparison = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'MultiQueryRetriever with Document Stuffing'])\n",
        "\n",
        "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
        "\n",
        "df_merged['Delta'] = df_merged['MultiQueryRetriever with Document Stuffing'] - df_merged['Baseline']\n",
        "\n",
        "df_merged"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "62NYn3iAvTjM",
        "outputId": "56bf11d6-14c0-456c-a982-03b8e70322ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Metric  Baseline  MultiQueryRetriever with Document Stuffing  \\\n",
              "0        faithfulness  0.881068                                    0.805291   \n",
              "1    answer_relevancy  0.937818                                    0.822632   \n",
              "2      context_recall  0.908736                                    0.938822   \n",
              "3   context_precision  0.804623                                    0.882976   \n",
              "4  answer_correctness  0.874418                                    0.872634   \n",
              "\n",
              "      Delta  \n",
              "0 -0.075776  \n",
              "1 -0.115186  \n",
              "2  0.030086  \n",
              "3  0.078353  \n",
              "4 -0.001784  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb8787d7-511f-460b-a9d9-4ef79771cea1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Baseline</th>\n",
              "      <th>MultiQueryRetriever with Document Stuffing</th>\n",
              "      <th>Delta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.881068</td>\n",
              "      <td>0.805291</td>\n",
              "      <td>-0.075776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.937818</td>\n",
              "      <td>0.822632</td>\n",
              "      <td>-0.115186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.908736</td>\n",
              "      <td>0.938822</td>\n",
              "      <td>0.030086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.804623</td>\n",
              "      <td>0.882976</td>\n",
              "      <td>0.078353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer_correctness</td>\n",
              "      <td>0.874418</td>\n",
              "      <td>0.872634</td>\n",
              "      <td>-0.001784</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb8787d7-511f-460b-a9d9-4ef79771cea1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fb8787d7-511f-460b-a9d9-4ef79771cea1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fb8787d7-511f-460b-a9d9-4ef79771cea1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-695993b8-bcbb-42fc-b8db-fe47055ebb37\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-695993b8-bcbb-42fc-b8db-fe47055ebb37')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-695993b8-bcbb-42fc-b8db-fe47055ebb37 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a104d22c-4244-4457-a911-a1a9c0b48185\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_merged')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a104d22c-4244-4457-a911-a1a9c0b48185 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_merged');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_merged",
              "summary": "{\n  \"name\": \"df_merged\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"answer_relevancy\",\n          \"answer_correctness\",\n          \"context_recall\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Baseline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04968350021193069,\n        \"min\": 0.8046233729698148,\n        \"max\": 0.9378183751271447,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9378183751271447,\n          0.874417555669557,\n          0.908736329226567\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MultiQueryRetriever with Document Stuffing\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05289111869920098,\n        \"min\": 0.8052914006550613,\n        \"max\": 0.938822195414749,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.822632062553319,\n          0.8726336817659698,\n          0.938822195414749\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Delta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07847897815423892,\n        \"min\": -0.11518631257382572,\n        \"max\": 0.07835250342836286,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.11518631257382572,\n          -0.001783873903587252,\n          0.030085866188181987\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s5VB24wN_yQM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}